{
	"name": "Azure ML Oj Sales Predictions",
	"properties": {
		"nbformat": 0,
		"nbformat_minor": 0,
		"bigDataPool": {
			"referenceName": "pythonspark",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2"
			}
		},
		"metadata": {
			"saveOutput": true,
			"synapse_widget": {
				"version": "0.1"
			},
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/e3047949-54a6-4aa8-b2bd-8e558bcc0850/resourceGroups/azure-nandu/providers/Microsoft.Synapse/workspaces/nandu-workspace/bigDataPools/pythonspark",
				"name": "pythonspark",
				"type": "Spark",
				"endpoint": "https://nandu-workspace.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/pythonspark",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "2.4",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			}
		},
		"cells": [
			{
				"cell_type": "markdown",
				"source": [
					"## Read data from Synapse table and register spark table\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%%spark\n",
					"\n",
					"// Read from Synapse SQL Pool and Create Spark Table\n",
					"\n",
					"spark.sql(\"CREATE DATABASE IF NOT EXISTS oj_sales\")\n",
					"val df = spark.read.sqlanalytics(\"aipsynpssqlpool.dbo.oj_sales\") \n",
					"df.write.mode(\"overwrite\").saveAsTable(\"oj_sales.oj_sales\")"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Read spark table into a dataframe\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"diagram": {
						"activateDiagramType": 1,
						"chartConfig": {
							"category": "bar",
							"keys": [
								"store"
							],
							"values": [
								"store"
							],
							"yLabel": "store",
							"xLabel": "store",
							"aggregation": "COUNT",
							"aggByBackend": false
						},
						"aggData": "{\"store\":{\"21\":102,\"45\":101,\"47\":180,\"48\":111,\"49\":95,\"50\":3,\"62\":68,\"71\":51,\"72\":97,\"73\":80,\"74\":100,\"95\":13}}",
						"isSummary": false,
						"previewData": {
							"filter": null
						},
						"isSql": false
					}
				},
				"source": [
					"# Load the oj data into DataFrame\n",
					"\n",
					"oj_df = spark.sql(\"SELECT * FROM oj_sales.oj_sales\")\n",
					"display(oj_df)"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Apply transformations and modify data types\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import col\n",
					"\n",
					"# Convert string column to float\n",
					"float_columns = ['logmove', 'price', 'AGE60', 'EDUC', 'ETHNIC', 'INCOME', 'HHLARGE', 'WORKWOM', 'HVAL150', 'SSTRDIST', 'SSTRVOL', 'CPDIST5', 'CPWVOL5']\n",
					"for col_name in float_columns:\n",
					"    oj_df = oj_df.withColumn(col_name,col(col_name).cast('float'))\n",
					"\n",
					"# Convert string column to int\n",
					"int_columns = ['store', 'week']\n",
					"for col_name in int_columns:\n",
					"    oj_df = oj_df.withColumn(col_name,col(col_name).cast('int'))\n",
					"\n",
					"# Convert string column to boolean\n",
					"oj_df = oj_df.withColumn(\"feat\",col(\"feat\").cast('boolean'))"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Split the original dataset into training and testing Data\n",
					"*The dataset contains data of 121 weeks starting from week 40 to week 160. Among this, 81 weeks data i.e. ranging from week 40 to week 120 is the training data while on the other hand, 39 weeks data ranging from 121 to 160 is testing data*"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Splitting the sales into training abd validation dataset based on week\n",
					"\n",
					"training_data = oj_df.filter(col(\"week\") <= 120)\n",
					"validation_data = oj_df.filter(col(\"week\") > 120)"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Create and configure Azure ML workspace\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from azureml.core.authentication import InteractiveLoginAuthentication\n",
					"from azureml.core import Workspace\n",
					"\n",
					"#Define workspace params(Parameters)\n",
					"tenant_id = \"9be10acf-2a10-42a0-9748-00a93cd07042\"\n",
					"subscription_id = \"cb9f6601-c384-4019-a885-22467caeec6d\" \n",
					"resource_group = \"AIP-WITH-SYNAPSE\" \n",
					"workspace_name = \"aipmlwrkspce\" \n",
					"workspace_region = \"East US\" \n",
					"\n",
					"# Create Workspace Object with above parameters\n",
					"interactive_auth = InteractiveLoginAuthentication(tenant_id=tenant_id)\n",
					"ws = Workspace(subscription_id=subscription_id,\n",
					"               resource_group=resource_group,\n",
					"               workspace_name=workspace_name,\n",
					"               auth=interactive_auth)\n",
					"\n",
					"# Persist/Write the workspace configuration details in a object's config\n",
					"ws = Workspace(workspace_name = workspace_name,\n",
					"               subscription_id = subscription_id,\n",
					"               resource_group = resource_group)\n",
					"ws.write_config()"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Upload training data to Azure ML Workspace's default datastore\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import pandas \n",
					"from azureml.core import Dataset\n",
					"\n",
					"# Get the AML Default Datastore\n",
					"datastore = ws.get_default_datastore()\n",
					"\n",
					"training_pd = training_data.toPandas().to_csv('training_pd.csv', index=False)\n",
					"\n",
					"# Convert into AML Tabular Dataset\n",
					"datastore.upload_files(files = ['training_pd.csv'],\n",
					"                       target_path = 'train-dataset/tabular/',\n",
					"                       overwrite = True,\n",
					"                       show_progress = True)\n",
					"\n",
					"dataset_training = Dataset.Tabular.from_delimited_files(path = [(datastore, 'train-dataset/tabular/training_pd.csv')])"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Set the experiment task as \"Regression\" for Auto ML and configure Auto ML parameters\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import logging\n",
					"from azureml.train.automl import AutoMLConfig\n",
					"\n",
					"automl_settings = {\n",
					"    \"iteration_timeout_minutes\": 10,\n",
					"    \"experiment_timeout_minutes\": 30,\n",
					"    \"enable_early_stopping\": True,\n",
					"    \"primary_metric\": 'r2_score',\n",
					"    \"featurization\": 'auto',\n",
					"    \"verbosity\": logging.INFO,\n",
					"    \"n_cross_validations\": 2}\n",
					"\n",
					"automl_config = AutoMLConfig(task='regression',\n",
					"                          debug_log='automated_ml_errors.log',\n",
					"                          training_data = dataset_training,\n",
					"                          spark_context = sc,\n",
					"                          model_explainability = False, \n",
					"                          label_column_name =\"logmove\",**automl_settings)"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Submit the Auto ML Experiment\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from azureml.core.experiment import Experiment\n",
					"\n",
					"# Start an experiment in Azure Machine Learning\n",
					"experiment = Experiment(ws, \"aml-synapse-regression\")\n",
					"tags = {\"Synapse\": \"regression\"}\n",
					"local_run = experiment.submit(automl_config, show_output=True, tags = tags)\n",
					"\n",
					"# Use the get_details function to retrieve the detailed output for the run.\n",
					"run_details = local_run.get_details()"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Get the best and fitted Model\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Get best model and fitted model\n",
					"best_run, fitted_model = local_run.get_output()"
				],
				"execution_count": 9
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Generate predictions by passing testing data"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"diagram": {
						"activateDiagramType": 1,
						"chartConfig": {
							"category": "bar",
							"keys": [
								"brand"
							],
							"values": [
								"store"
							],
							"yLabel": "store",
							"xLabel": "brand",
							"aggregation": "SUM",
							"aggByBackend": false
						},
						"aggData": "{\"store\":{\"dominicks\":15805,\"minute.maid\":40964,\"tropicana\":29480}}",
						"isSummary": false,
						"previewData": {
							"filter": null
						},
						"isSql": false
					}
				},
				"source": [
					"# Generate Predictions on test data\n",
					"validation_data_pd = validation_data.toPandas()\n",
					"y_test = validation_data_pd.pop(\"logmove\").to_frame()\n",
					"y_predict = fitted_model.predict(validation_data_pd)\n",
					"validation_data_pd[\"predicted_logmove\"] = y_predict\n",
					"display(validation_data_pd)"
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Merge the predicted data with actual data"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"diagram": {
						"activateDiagramType": 1,
						"chartConfig": {
							"category": "bar",
							"keys": [
								"brand"
							],
							"values": [
								"store"
							],
							"yLabel": "store",
							"xLabel": "brand",
							"aggregation": "SUM",
							"aggByBackend": false
						},
						"aggData": "{\"store\":{\"dominicks\":30877,\"minute.maid\":28750,\"tropicana\":7749}}",
						"isSummary": false,
						"previewData": {
							"filter": null
						},
						"isSql": false
					}
				},
				"source": [
					"# Write the predicted df\n",
					"from pyspark.sql.functions import *\n",
					"\n",
					"column_list = ['store', 'brand', 'week', 'predicted_logmove', 'feat', 'price', 'AGE60', 'EDUC', 'ETHNIC', 'INCOME', 'HHLARGE', 'WORKWOM', 'HVAL150', 'SSTRDIST', 'SSTRVOL', 'CPDIST5', 'CPWVOL5', 'VALTYPE']\n",
					"\n",
					"# Convert Pandas DataFrame back to Spark\n",
					"validation_df = spark.createDataFrame(validation_data_pd)\n",
					"\n",
					"# Add Value Type Column in both Historical Data and Predicted Data to identify predicted rows\n",
					"training_data = training_data.withColumn(\"VALTYPE\",lit(\"Actual\"))\n",
					"validation_df = (validation_df.withColumn(\"VALTYPE\",lit(\"Predicted\"))).select(column_list)\n",
					"\n",
					"# Union training and testing data\n",
					"aml_df = training_data.union(validation_df)\n",
					"\n",
					"# Save the final DataFrame as Spark table\n",
					"aml_df.write.saveAsTable(\"aml_predicted_oj_sales\",mode=\"overwrite\")\n",
					"display(aml_df)"
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Write the final dataframe to Synapse SQL Pool\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"%%spark\n",
					"\n",
					"// Write the final table to Synapse SQL Pool for Power BI Reporting\n",
					"\n",
					"val scala_df = spark.sqlContext.sql (\"select * from aml_predicted_oj_sales\")\n",
					"\n",
					"scala_df.write.mode(\"overwrite\").sqlanalytics(\"aipsynpssqlpool.dbo.aml_predicted_oj_sales\", Constants.INTERNAL)"
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Calculate R Squared Value (R2)\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.ml.evaluation import RegressionEvaluator\n",
					"\n",
					"# Prepare validation data\n",
					"val_pd = validation_data_pd\n",
					"val_pd[\"logmove\"] = y_test\n",
					"val_df = spark.createDataFrame(val_pd)\n",
					"\n",
					"# Find R Squared (R2)\n",
					"lr_evaluator = RegressionEvaluator(predictionCol=\"predicted_logmove\", \\\n",
					"                 labelCol=\"logmove\",metricName=\"r2\")\n",
					"print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(val_df))"
				],
				"execution_count": 13
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Evaluate our model with Root Mean Square Error\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"from sklearn.metrics import mean_squared_error\n",
					"from math import sqrt\n",
					"\n",
					"# Calculate Root Mean Square Error\n",
					"y_actual = y_test.values.flatten().tolist()\n",
					"rmse = sqrt(mean_squared_error(y_actual, y_predict))\n",
					"\n",
					"print(\"Root Mean Square Error:\")\n",
					"print(rmse)"
				],
				"execution_count": 14
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Find Mean Abs Percent Error and Accuracy of the model\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Calculate MAPE and Model Accuracy \n",
					"sum_actuals = sum_errors = 0\n",
					"\n",
					"for actual_val, predict_val in zip(y_actual, y_predict):\n",
					"    abs_error = actual_val - predict_val\n",
					"    if abs_error < 0:\n",
					"        abs_error = abs_error * -1\n",
					"\n",
					"    sum_errors = sum_errors + abs_error\n",
					"    sum_actuals = sum_actuals + actual_val\n",
					"\n",
					"mean_abs_percent_error = sum_errors / sum_actuals\n",
					"\n",
					"print(\"Model MAPE:\")\n",
					"print(mean_abs_percent_error)\n",
					"print()\n",
					"print(\"Model Accuracy:\")\n",
					"print(1 - mean_abs_percent_error)"
				],
				"execution_count": 15
			},
			{
				"cell_type": "markdown",
				"source": [
					"## Register the best run model in the Azure ML workspace\n",
					""
				]
			},
			{
				"cell_type": "code",
				"source": [
					"description = 'My AutoML Model'\n",
					"model_path='outputs/model.pkl'\n",
					"model = best_run.register_model(model_name = 'aml_oj_sales_model', model_path = model_path, description = description)\n",
					"print(model.name, model.version)"
				],
				"execution_count": 16
			}
		]
	}
}